{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-24T17:18:36.944581Z",
     "iopub.status.busy": "2023-05-24T17:18:36.943880Z",
     "iopub.status.idle": "2023-05-24T17:18:36.949105Z",
     "shell.execute_reply": "2023-05-24T17:18:36.948246Z",
     "shell.execute_reply.started": "2023-05-24T17:18:36.944543Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, ELU, Softmax, Flatten, Conv2D\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:18:37.086673Z",
     "iopub.status.busy": "2023-05-24T17:18:37.086322Z",
     "iopub.status.idle": "2023-05-24T17:18:37.091279Z",
     "shell.execute_reply": "2023-05-24T17:18:37.090354Z",
     "shell.execute_reply.started": "2023-05-24T17:18:37.086645Z"
    }
   },
   "outputs": [],
   "source": [
    "model_iteration = 0\n",
    "number_epochs = 10\n",
    "batch_size = 2\n",
    "my_lr = 1e-5\n",
    "easy_mode = True\n",
    "\n",
    "#dataset_name = \"rock_paper_scissors\"\n",
    "dataset_name = \"beans\"\n",
    "\n",
    "model_name = \"mobilenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfds.load(dataset_name, split=\"train\", shuffle_files=True)\n",
    "test_dataset = tfds.load(dataset_name, split=\"test\", shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfds_to_numpy(dataset):\n",
    "    \n",
    "    train_np_iterator = tfds.as_numpy(dataset)\n",
    "\n",
    "    train_x = None\n",
    "    train_y = None\n",
    "\n",
    "    for elem in train_np_iterator:\n",
    "\n",
    "        image = elem[\"image\"].reshape(-1, *elem[\"image\"].shape)\n",
    "        label = elem[\"label\"].reshape(1)\n",
    "        if train_x is None:\n",
    "            train_x = image\n",
    "            train_y = label\n",
    "        else:\n",
    "            train_x = np.append(train_x, image, 0)\n",
    "            train_y = np.append(train_y, label, 0)\n",
    "        \n",
    "        \n",
    "    return train_x, train_y\n",
    "\n",
    "test_x, test_y = tfds_to_numpy(test_dataset)\n",
    "print(test_x.shape)\n",
    "train_x, train_y = tfds_to_numpy(train_dataset)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:31:23.414676Z",
     "iopub.status.busy": "2023-05-24T17:31:23.414216Z",
     "iopub.status.idle": "2023-05-24T17:31:29.678470Z",
     "shell.execute_reply": "2023-05-24T17:31:29.677541Z",
     "shell.execute_reply.started": "2023-05-24T17:31:23.414642Z"
    }
   },
   "outputs": [],
   "source": [
    "image = train_x[:1]\n",
    "number_classes = np.max(train_y)+ 1 ##np.max(train_dataset[1]) + 1\n",
    "input_shape = image.shape[1:]\n",
    "\n",
    "def initialize_model(number_classes=number_classes, \\\n",
    "        input_shape=input_shape, hidden_dims=64, my_lr=my_lr):\n",
    "\n",
    "    tf.random.set_seed(13)\n",
    "    np.random.seed(13)\n",
    "    \n",
    "    extractor = tf.keras.applications.MobileNet(\\\n",
    "        input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "\n",
    "    # set to True to also train the feature extraction layers\n",
    "    extractor.trainable = False\n",
    "    \n",
    "    model = Sequential([extractor, \\\n",
    "                        Flatten(), \\\n",
    "                        Dropout(0.25), \\\n",
    "                        Dense(hidden_dims), \\\n",
    "                        ELU(), \\\n",
    "                        Dropout(0.25), \\\n",
    "                        Dense(hidden_dims), \\\n",
    "                        ELU(), \\\n",
    "                        Dense(number_classes),\n",
    "                        Softmax()\n",
    "                       ])\n",
    "\n",
    "    example_output = model(np.random.rand(1,*input_shape))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\\\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics = ['accuracy']\n",
    "                 )\n",
    "\n",
    "    model.optimizer.lr = my_lr\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:19:21.475496Z",
     "iopub.status.busy": "2023-05-24T17:19:21.475191Z",
     "iopub.status.idle": "2023-05-24T17:19:21.497255Z",
     "shell.execute_reply": "2023-05-24T17:19:21.495921Z",
     "shell.execute_reply.started": "2023-05-24T17:19:21.475472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory for saving model as SavedModel \n",
    "saved_model_dir = os.path.join(\\\n",
    "        \"..\", \"models\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "\n",
    "if os.path.exists(saved_model_dir):\n",
    "    while os.path.exists(saved_model_dir):\n",
    "        model_iteration += 1\n",
    "        saved_model_dir = os.path.join(\\\n",
    "                \"..\", \"models\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "else:\n",
    "    os.system(f\"mkdir {saved_model_dir} -p\")\n",
    "\n",
    "# Directory for saving model using BackupAndRestore\n",
    "saved_backup_dir = os.path.join(\\\n",
    "        \"..\", \"backups\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "\n",
    "if os.path.exists(saved_backup_dir):\n",
    "    while os.path.exists(saved_backup_dir):\n",
    "        model_iteration += 1\n",
    "        saved_backup_dir = os.path.join(\\\n",
    "                \"..\", \"backups\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "else:\n",
    "    os.system(f\"mkdir {saved_backup_dir} -p\")\n",
    "    \n",
    "saved_backup_path = os.path.join(saved_backup_dir, \"backup_{epoch:03d}_{val_loss:.2f}.ckpt\")\n",
    "    \n",
    "\n",
    "# Directory for saving weights (only)\n",
    "saved_weights_dir = os.path.join(\\\n",
    "        \"..\", \"weights\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "\n",
    "if os.path.exists(saved_weights_dir):\n",
    "    while os.path.exists(saved_weights_dir):\n",
    "        model_iteration += 1\n",
    "        saved_weights_dir = os.path.join(\\\n",
    "                \"..\", \"weights\",  f\"{dataset_name}_{model_name}{model_iteration:03}\")\n",
    "else:\n",
    "    os.system(f\"mkdir {saved_weights_dir} -p\")\n",
    "\n",
    "saved_weights_path = os.path.join(saved_weights_dir, \"checkpoint_{epoch:03d}_{val_loss:.2f}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:30:23.341134Z",
     "iopub.status.busy": "2023-05-24T17:30:23.340769Z",
     "iopub.status.idle": "2023-05-24T17:30:23.347163Z",
     "shell.execute_reply": "2023-05-24T17:30:23.346276Z",
     "shell.execute_reply.started": "2023-05-24T17:30:23.341106Z"
    }
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\\\n",
    "    log_dir=\"logs\", \\\n",
    "    write_graph=False, \\\n",
    "    update_freq='epoch', \\\n",
    ")\n",
    "\n",
    "# This is where we will instantiate a checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    saved_weights_path, \\\n",
    "    save_freq = \"epoch\", \\\n",
    "    monitor =  'val_accuracy', \\\n",
    "    save_weights_only = True, \\\n",
    "    verbose = 1)\n",
    "\n",
    "# This is where we will define a BackupAndRestore callback\n",
    "backup_callback = tf.keras.callbacks.BackupAndRestore( \\\n",
    "    saved_backup_dir, \\\n",
    "    save_freq = \"epoch\", \\\n",
    "    delete_checkpoint = True, \\\n",
    "    save_before_preemption = False\n",
    "    )\n",
    "\n",
    "\n",
    "# Defining a deliberately deleterious learning rate scheduler\n",
    "def scheduler(epoch, lr, epochs=number_epochs):\n",
    "    \n",
    "    if epoch <= max([1, epochs - epochs // 4]):\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 20.\n",
    "\n",
    "# A callback to interrupt training, to demonstrate BackupAndRestore utility\n",
    "class Interrupt(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == 2:\n",
    "            print(\"\\n Interrupting callback\")\n",
    "            raise RuntimeError(\"Interrupting callback who?\")\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "interrupt_callback = Interrupt()\n",
    "\n",
    "basic_callbacks = [tensorboard_callback, lr_callback]\n",
    "\n",
    "callbacks_with_interrupt = basic_callbacks + [interrupt_callback]\n",
    "callbacks_with_backup = callbacks_with_interrupt + [backup_callback]\n",
    "\n",
    "callbacks_with_checkpoints = basic_callbacks + [checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:31:11.373111Z",
     "iopub.status.busy": "2023-05-24T17:31:11.372755Z",
     "iopub.status.idle": "2023-05-24T17:31:11.422375Z",
     "shell.execute_reply": "2023-05-24T17:31:11.420159Z",
     "shell.execute_reply.started": "2023-05-24T17:31:11.373082Z"
    }
   },
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "try:\n",
    "    history = model.fit(x=train_x, y=train_y, \\\n",
    "        validation_split=0.1, \\\n",
    "        batch_size=batch_size, epochs=5, \\\n",
    "        callbacks=callbacks_with_interrupt)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "history = model.fit(x=train_x, y=train_y, \\\n",
    "        validation_split=0.1, \\\n",
    "        batch_size=batch_size, epochs=5, \\\n",
    "        callbacks=basic_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "try:\n",
    "    history = model.fit(x=train_x, y=train_y, \\\n",
    "        validation_split=0.1, \\\n",
    "        batch_size=batch_size, epochs=5, \\\n",
    "        callbacks=callbacks_with_backup)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "history = model.fit(x=train_x, y=train_y, \\\n",
    "        validation_split=0.1, \\\n",
    "        batch_size=batch_size, epochs=5, \\\n",
    "        callbacks=basic_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:33:17.167576Z",
     "iopub.status.busy": "2023-05-24T17:33:17.167208Z",
     "iopub.status.idle": "2023-05-24T17:33:46.404202Z",
     "shell.execute_reply": "2023-05-24T17:33:46.403197Z",
     "shell.execute_reply.started": "2023-05-24T17:33:17.167545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model weights manually\n",
    "weights_path = f\"manual_weights{model_iteration:03}.ckpt\"\n",
    "\n",
    "model.save_weights(os.path.join(saved_weights_dir, weights_path))\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "\n",
    "print(f\"model loss: {loss:.3e},  accuracy: {accuracy:.3f}\")\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(f\"newly instantiated model loss: {loss:.3e},  accuracy: {accuracy:.3f}\")\n",
    "\n",
    "model.load_weights(os.path.join(saved_weights_dir, weights_path))\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "\n",
    "print(f\"model (weights loaded from disk) loss: {loss:.3e},  accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(x=train_x, y=train_y, \\\n",
    "        validation_split=0.1, \\\n",
    "        batch_size=batch_size, epochs=number_epochs, \\\n",
    "        callbacks=callbacks_with_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training\n",
    "\n",
    "my_cmap = plt.get_cmap(\"magma\")\n",
    "loss_color = my_cmap(16)\n",
    "val_loss_color = my_cmap(64)\n",
    "\n",
    "my_cmap = plt.get_cmap(\"viridis\")\n",
    "acc_color = my_cmap(128)\n",
    "val_acc_color = my_cmap(192)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,4))\n",
    "ax_twin = ax.twinx()\n",
    "\n",
    "ax.plot(history.history[\"loss\"], color=loss_color, label=\"training loss\")\n",
    "ax.plot(history.history[\"val_loss\"], color=val_loss_color, label=\"validation loss\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_yticks(np.arange(0,4.0,0.5))\n",
    "\n",
    "ax_twin.plot(history.history[\"accuracy\"],color=acc_color, label=\"training accuracy\")\n",
    "ax_twin.plot(history.history[\"val_accuracy\"], color=val_acc_color, label=\"validation accuracy\")\n",
    "ax_twin.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax_twin.set_yticks(np.arange(0,1.0,0.1))\n",
    "ax.legend(loc=6)\n",
    "\n",
    "ax_twin.legend(loc=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights_dir, weights_path, saved_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint with best performance\n",
    "\n",
    "# add checkpoint with good performance\n",
    "good_checkpoint = 6\n",
    "chkpt_listdir = os.listdir(saved_weights_dir)\n",
    "\n",
    "for elem in chkpt_listdir:\n",
    "    if f\"checkpoint_{good_checkpoint:03}\" in elem and \"index\" not in elem:\n",
    "        \n",
    "        print(elem)\n",
    "        load_it = elem\n",
    "        \n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "\n",
    "print(f\"model (final training step) loss: {loss:.3e},  accuracy: {accuracy:.3f}\")\n",
    "\n",
    "good_checkpoint_path = os.path.join(saved_weights_dir, load_it)\n",
    "model.load_weights(good_checkpoint_path) \n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "\n",
    "print(f\"model (loaded from last good checkpoint) loss: {loss:.3e},  accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T17:36:16.673269Z",
     "iopub.status.busy": "2023-05-24T17:36:16.672899Z",
     "iopub.status.idle": "2023-05-24T17:36:25.611732Z",
     "shell.execute_reply": "2023-05-24T17:36:25.610406Z",
     "shell.execute_reply.started": "2023-05-24T17:36:16.673239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model weights and architecture in SavedModel format\n",
    "# If we change the model architecture but try to load the weights we saved before\n",
    "wrong_model = initialize_model(hidden_dims=256)\n",
    "\n",
    "print(\"Trying to load weights, into an architecture that does not match\")\n",
    "wrong_model.load_weights(good_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the full Keras Model object, including architecture and optimizer state\n",
    "\n",
    "model.save(saved_model_dir)\n",
    "print(\"model saved\")\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "print(f\"model that was saved to SavedModel directory, loss: {loss:.3e},  accuracy: {accuracy:.3f}\")\n",
    "\n",
    "restored_model = tf.keras.models.load_model(saved_model_dir)\n",
    "print(\"model restored\")\n",
    "\n",
    "loss, accuracy = restored_model.evaluate(test_x, test_y, \\\n",
    "        batch_size=batch_size)\n",
    "print(f\"restored from SavedModel directory, loss: {loss:.3e},  accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Keras SavedModel to a TF Lite model\n",
    "\n",
    "model.save(saved_model_dir)\n",
    "print(f\"model saved to {saved_model_dir}\")\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tf_lite_model = converter.convert()\n",
    "\n",
    "tf_lite_path = os.path.join(\\\n",
    "        \"..\", \"models\",  f\"{dataset_name}_tf_lite\")\n",
    "\n",
    "# Save the model.\n",
    "with open(tf_lite_path, \"wb\") as f:\n",
    "    f.write(tf_lite_model)\n",
    "\n",
    "print(f\"tf lite model saved to {tf_lite_path}\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=tf_lite_path)\n",
    "\n",
    "\n",
    "signatures = interpreter.get_signature_list()\n",
    "\n",
    "\n",
    "tf_lite_signature = interpreter.get_signature_runner()\n",
    "input_details = tf_lite_signature.get_input_details()\n",
    "output_details = tf_lite_signature.get_output_details()\n",
    "\n",
    "print(f\"TF Lite signatures {signatures}\")\n",
    "print(f\"TF Lite input details {input_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_lite = 0\n",
    "total_samples = test_x.shape[0]\n",
    "\n",
    "for my_index in range(test_x.shape[0]):\n",
    "\n",
    "    dtype = input_details[list(input_details.keys())[0]][\"dtype\"]\n",
    "    my_batch = np.array(test_x[my_index:my_index+1], dtype=dtype)\n",
    "    \n",
    "    full_output_data = model(my_batch)\n",
    "    input_name = list(input_details.keys())[0] \n",
    "    output_name = list(output_details.keys())[0]\n",
    "    \n",
    "    output_data = tf_lite_signature(**{input_name: my_batch})[output_name]                   \n",
    "    \n",
    "    true_label = test_y[my_index]\n",
    "    \n",
    "    correct_lite += 1.0 * (output_data.argmax() == true_label)\n",
    "    \n",
    "accuracy_lite = correct_lite / total_samples\n",
    "\n",
    "print(f\"TF Lite test accuracy: {accuracy_lite}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to TensorFlow JS\n",
    "# to install tensorflowjs:\n",
    "# ! pip install tensorflowjs\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# convert from the keras model directly\n",
    "tfjs.converters.save_keras_model(model, \"tfjs_from_model\")\n",
    "\n",
    "# convert from the SavedModel directory\n",
    "tfjs.converters.convert_tf_saved_model(saved_model_dir, \"tfjs_from_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the contents of the tfjs directories\n",
    "! ls tfjs_from_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls tfjs_from_saved_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
